{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Visual Question Answering**"
      ],
      "metadata": {
        "id": "rI42oSKG8ruh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook presents an approach to solving the Visual Question Answering (VQA) task by leveraging a pretrained **ResNet** model to extract image embeddings, combined with various models for processing the textual component of the input."
      ],
      "metadata": {
        "id": "FhiNrXqq_WtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import libraries**"
      ],
      "metadata": {
        "id": "w4ririTg8w-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standard libs\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "from datetime import datetime as dt # timing\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# ResNet\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "# TorchVision\n",
        "import torchvision\n",
        "from torchvision import transforms, utils, models\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# TensorBoard for logging\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "f9m9So-t-8yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data preprocessing**"
      ],
      "metadata": {
        "id": "7azbG3GDDSJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QecfHx1w_JSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 24\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# path\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "new_folder = 'ResNet50'\n",
        "full_path = os.path.join(drive_path, new_folder)\n",
        "\n",
        "os.makedirs(full_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "rJ4st-4YwEe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions for loading the data\n",
        "\n",
        "def load_split_lists(folder_path, train_filename=\"train_list.pkl\", val_filename=\"val_list.pkl\"):\n",
        "    \"\"\"\n",
        "    Loads training and validation list data from pickle files.\n",
        "    \"\"\"\n",
        "    train_path = os.path.join(folder_path, train_filename)\n",
        "    val_path = os.path.join(folder_path, val_filename)\n",
        "\n",
        "    def load_list(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    train_list = load_list(train_path)\n",
        "    val_list = load_list(val_path)\n",
        "\n",
        "    print(f\"Loaded {len(train_list)} training items.\")\n",
        "    print(f\"Loaded {len(val_list)} validation items.\")\n",
        "\n",
        "    return train_list, val_list\n",
        "\n",
        "def load_features_by_split(split_name, folder_path):\n",
        "    \"\"\"\n",
        "    Loads image features and corresponding image IDs for a given split.\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(folder_path, f\"{split_name}_image_features.pkl\")\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    img_ids = data[\"img_ids\"]\n",
        "    features = data[\"features\"]\n",
        "    print(f\"Loaded features for {split_name}: {len(img_ids)} items.\")\n",
        "    return img_ids, features"
      ],
      "metadata": {
        "id": "Ju1Wl2QZwSdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encode questions and answers**"
      ],
      "metadata": {
        "id": "gw17mN-H9Dha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bert\n",
        "global_bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(f\"BERT tokenizer loaded for pre-processing. Vocabulary size: {global_bert_tokenizer.vocab_size}\")\n",
        "\n",
        "# used for padding_idx\n",
        "question_vocab = None\n",
        "\n",
        "def create_question_vocab(qa_list):\n",
        "    '''\n",
        "    Creates vocabulary for given json formatted dataset using BERT's tokenizer to get tokens.\n",
        "    '''\n",
        "    # Initialize with special tokens\n",
        "    vocab = ['<PAD>', '<UNK>']\n",
        "\n",
        "    unique_tokens = set()\n",
        "    for q_item in qa_list:\n",
        "        bert_tokens = global_bert_tokenizer.tokenize(q_item['question'].lower())\n",
        "        unique_tokens.update(bert_tokens)\n",
        "\n",
        "    vocab.extend(sorted(list(unique_tokens)))\n",
        "\n",
        "    vocab_dict = {}\n",
        "    for i in range(len(vocab)):\n",
        "        vocab_dict[vocab[i]] = i\n",
        "\n",
        "    print(f\"Created custom question vocabulary with {len(vocab_dict)} entries.\")\n",
        "    return vocab_dict\n",
        "\n",
        "def encode_questions(qa_list, question_vocab, max_length = 25):\n",
        "    '''\n",
        "    Given data json and custom question vocab. This function encodes each question into 25 length list\n",
        "    using BERT tokenizer and then mapping to custom vocab IDs.\n",
        "    '''\n",
        "    encoded_questions = []\n",
        "\n",
        "    for q_item in qa_list:\n",
        "        # tokeniser bert\n",
        "        bert_tokens = global_bert_tokenizer.tokenize(q_item['question'].lower())\n",
        "\n",
        "        # tokens in vocab id's\n",
        "        question_ids = [question_vocab.get(token, question_vocab['<UNK>']) for token in bert_tokens]\n",
        "\n",
        "        # padding\n",
        "        length = len(question_ids)\n",
        "        if(length < max_length):\n",
        "            question_ids += [question_vocab['<PAD>'] for i in range(max_length - length)]\n",
        "        else:\n",
        "            question_ids = question_ids[:max_length]\n",
        "\n",
        "        encoded_questions.append(question_ids)\n",
        "    return encoded_questions\n",
        "\n",
        "\n",
        "def create_answer_vocab(qalist, top=1000):\n",
        "    counts = {}\n",
        "    for annotation in qalist:\n",
        "        for answer in annotation['answers']:\n",
        "            a = answer['answer'].lower()\n",
        "            counts[a] = counts.get(a, 0) + 1\n",
        "        a = annotation['multiple_choice_answer'].lower()\n",
        "        counts[a] = counts.get(a, 0) + 1\n",
        "\n",
        "    sorted_ans = sorted([(count, ans) for ans, count in counts.items()], reverse=True)\n",
        "    answers_list = [sorted_ans[i][1] for i in range(min(top - 1, len(sorted_ans)))]\n",
        "    if '<UNK>' not in answers_list:\n",
        "        answers_list.append('<UNK>')\n",
        "\n",
        "    return {ans: i for i, ans in enumerate(answers_list)}\n",
        "\n",
        "def encode_answers(qa_list, answer_vocab):\n",
        "    '''\n",
        "    returns list of indexes corresponding to top_answers\n",
        "    '''\n",
        "    encoded_answers = []\n",
        "    encoded_multi_answers = []\n",
        "    # get UNK ID\n",
        "    unk_answer_id = answer_vocab.get('<UNK>', len(answer_vocab) - 1)\n",
        "\n",
        "    for annotation in qa_list:\n",
        "        all_answers = list(set([answer[\"answer\"].lower() for answer in annotation['answers']]))\n",
        "\n",
        "        valid_answers = [a for a in all_answers if a in answer_vocab]\n",
        "        if not valid_answers:\n",
        "            valid_answers = ['<UNK>']\n",
        "\n",
        "        answers_ids = [answer_vocab[answer] for answer in valid_answers]\n",
        "\n",
        "        # multi_answers to a fixed length based on batch\n",
        "        multi_answers_padded = [-11]*10\n",
        "        multi_answers_padded[:len(answers_ids)] = answers_ids\n",
        "        encoded_multi_answers.append(multi_answers_padded)\n",
        "\n",
        "        primary_answer_id = answer_vocab.get(annotation['multiple_choice_answer'].lower(), unk_answer_id)\n",
        "\n",
        "        # consensus answer is UNK, but other annotators gave valid answers, pick one of those\n",
        "        if primary_answer_id == unk_answer_id and answers_ids:\n",
        "            valid_non_unk_answers = [aid for aid in answers_ids if aid != unk_answer_id]\n",
        "            if valid_non_unk_answers:\n",
        "                encoded_answers.append(np.random.choice(valid_non_unk_answers))\n",
        "            else:\n",
        "                encoded_answers.append(unk_answer_id)\n",
        "        else:\n",
        "            encoded_answers.append(primary_answer_id)\n",
        "\n",
        "    return encoded_answers, np.array(encoded_multi_answers)\n",
        "\n",
        "def filter_dataset(qa_list, answer_type):\n",
        "    '''\n",
        "    filters the dataset based on the given answer type\n",
        "    '''\n",
        "    return [x for x in qa_list if(x['answer_type'] == answer_type) ]"
      ],
      "metadata": {
        "id": "UAriYpSxwb_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create dataset**"
      ],
      "metadata": {
        "id": "Vzbynyys9Mek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VQA Dataset Class\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, json_list, question_vocab, answer_vocab, split, train_img_feats=None, val_img_feats=None, answer_type=None, max_question_length=25):\n",
        "        \"\"\"\n",
        "          json_list (list): List of question-answer dictionaries.\n",
        "          question_vocab (dict): The vocabulary mapping question tokens to IDs.\n",
        "          answer_vocab (dict): The vocabulary mapping answers to IDs.\n",
        "          split (str): 'train' or 'val' to determine which image features to use.\n",
        "          train_img_feats (tuple): (img_ids, features) for training images.\n",
        "          val_img_feats (tuple): (img_ids, features) for validation images.\n",
        "          answer_type (str, optional): Filters the dataset by answer type (e.g., 'yes/no').\n",
        "          max_question_length (int): Max length for question tokenization.\n",
        "        \"\"\"\n",
        "        if answer_type is not None:\n",
        "            self.text_data = filter_dataset(json_list, answer_type)\n",
        "            print(f\"Dataset filtered for answer_type: '{answer_type}'. New size: {len(self.text_data)}\")\n",
        "        else:\n",
        "            self.text_data = json_list\n",
        "\n",
        "        self.questions_encoded = encode_questions(self.text_data, question_vocab, max_length=max_question_length)\n",
        "        self.answers_encoded, self.multi_answers = encode_answers(self.text_data, answer_vocab)\n",
        "\n",
        "        if split == 'train':\n",
        "            self.img_ids_list, self.img_features = train_img_feats\n",
        "        elif split == 'val':\n",
        "            self.img_ids_list, self.img_features = val_img_feats\n",
        "\n",
        "        # map image_id -> index for quick lookup\n",
        "        self.img_id_to_idx = {img_id: idx for idx, img_id in enumerate(self.img_ids_list)}\n",
        "\n",
        "    def __len__(self):\n",
        "        # length\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        take an item from the dataset at the specified index\n",
        "\n",
        "        index (int): The index of the item to retrieve.\n",
        "\n",
        "        \"\"\"\n",
        "        img_id = self.text_data[index]['image_id']\n",
        "        img_idx = self.img_id_to_idx.get(img_id, None)\n",
        "\n",
        "        if img_idx is None:\n",
        "            print(f\"Warning: Image ID {img_id} not found in pre-loaded image features for index {index}. Returning dummy data.\")\n",
        "            dummy_image_feature = torch.zeros(self.img_features.shape[1], dtype=torch.float32)\n",
        "            item_image = dummy_image_feature\n",
        "        else:\n",
        "            item_image = self.img_features[img_idx]\n",
        "\n",
        "        item = {}\n",
        "        item['image'] = item_image\n",
        "        item['question'] = torch.tensor(self.questions_encoded[index], dtype=torch.long)\n",
        "        item['answer'] = torch.tensor(self.answers_encoded[index], dtype=torch.long)\n",
        "        item['multi_answer'] = torch.tensor(self.multi_answers[index], dtype=torch.long)\n",
        "\n",
        "        # return dictionary containing 'image', 'question', 'answer', and 'multi_answer' tensors.\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "WUvK3JJr4TpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deeper LSTM**"
      ],
      "metadata": {
        "id": "pNGgb6gJDlFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, img_dim, output_dim, dropout_p = 0.5):\n",
        "\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        # initializing the layers\n",
        "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx = question_vocab['<PAD>'])\n",
        "        self.fc_embed = nn.Linear(300, 300)\n",
        "        self.lstm = nn.LSTM(300, 512, num_layers =2, batch_first = True)\n",
        "        self.fc_lstm = nn.Linear(4*512, 1024)\n",
        "        self.fc_i = nn.Linear(img_dim, 1024)\n",
        "        self.fc1 = nn.Linear(1024, 1000)\n",
        "        self.fc2 = nn.Linear(1000, output_dim) # dynamically matches answer vocab size\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # initializing weights\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def forward(self, img_feat_input, text_input):\n",
        "\n",
        "        # with gelu non-linearity\n",
        "        text_embedded = F.gelu(self.fc_embed(self.embedding(text_input)))\n",
        "\n",
        "        _, (lstm_hidden, lstm_cell)  = self.lstm(text_embedded)\n",
        "\n",
        "        # concat\n",
        "        lstm_concat = torch.cat([lstm_hidden[0], lstm_hidden[1], lstm_cell[0], lstm_cell[1]], 1)\n",
        "\n",
        "        # fully connected with gelu non-linearity\n",
        "        fc_lstm = self.dropout(F.gelu(self.fc_lstm(lstm_concat)))\n",
        "\n",
        "        # l2 normalization\n",
        "        fc_i = F.gelu(self.fc_i(F.normalize(img_feat_input, dim = 1, p=2)))\n",
        "\n",
        "        # point-wise multiplication\n",
        "        pw_mul = fc_i * fc_lstm\n",
        "\n",
        "        # dropout\n",
        "        fc_1 = self.dropout(F.gelu(self.fc1(self.dropout(pw_mul))))\n",
        "\n",
        "        # output\n",
        "        output = F.softmax(self.fc2(fc_1), dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def init_weights(self):\n",
        "\n",
        "        # kaiming_uniform\n",
        "        init = torch.nn.init.kaiming_uniform_\n",
        "        for layer in self.lstm.all_weights:\n",
        "            for hidden in layer[:2]:\n",
        "                init(hidden)\n",
        "\n",
        "        init(self.fc_embed.weight)\n",
        "        init(self.fc_lstm.weight)\n",
        "        init(self.fc_i.weight)\n",
        "        init(self.fc1.weight)\n",
        "        init(self.fc2.weight)"
      ],
      "metadata": {
        "id": "QEI4AxQ9wh6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare for training**"
      ],
      "metadata": {
        "id": "pVdMjqcH9VqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "num_workers = 2\n",
        "batch_size = 512\n",
        "epochs = 15\n",
        "\n",
        "# data loading\n",
        "train_list, val_list = load_split_lists(full_path)\n",
        "\n",
        "train_img_ids, train_features_np = load_features_by_split(\"train2014\", full_path)\n",
        "val_img_ids, val_features_np = load_features_by_split(\"val2014\", full_path)\n",
        "\n",
        "train_features = torch.tensor(train_features_np, dtype=torch.float32)\n",
        "val_features = torch.tensor(val_features_np, dtype=torch.float32)\n",
        "\n",
        "actual_img_dim = train_features.shape[1]\n",
        "print(f\"Detected image feature dimension (img_dim): {actual_img_dim}\")"
      ],
      "metadata": {
        "id": "XCLhhrUz5byE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vocabularies\n",
        "question_vocab = create_question_vocab(train_list)\n",
        "answer_vocab = create_answer_vocab(train_list, top=1000)  # top-k answers\n",
        "output_vocab_size = len(answer_vocab)\n",
        "\n",
        "print(f\"Question vocab size: {len(question_vocab)}\")\n",
        "print(f\"Answer vocab size: {output_vocab_size}\")"
      ],
      "metadata": {
        "id": "3g5iXQl35sjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize datasets\n",
        "train_dataset = VQADataset(json_list=train_list,\n",
        "                             question_vocab=question_vocab,\n",
        "                             answer_vocab=answer_vocab,\n",
        "                             split='train',\n",
        "                             train_img_feats=(train_img_ids, train_features))\n",
        "\n",
        "val_dataset = VQADataset(json_list=val_list,\n",
        "                            question_vocab=question_vocab,\n",
        "                            answer_vocab=answer_vocab,\n",
        "                            split='val',\n",
        "                            val_img_feats=(val_img_ids, val_features))\n",
        "\n",
        "# free up memory after dataset creation\n",
        "del train_list\n",
        "del val_list\n",
        "del train_features_np\n",
        "del val_features_np\n",
        "del train_features\n",
        "del val_features\n",
        "print(\"Data loaded and datasets initialized. Memory for raw data cleared.\")\n",
        "\n",
        "# initialize DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=num_workers,\n",
        "                            pin_memory=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# model initialization\n",
        "model = LSTM(\n",
        "    vocab_size=len(question_vocab),\n",
        "    img_dim=actual_img_dim,\n",
        "    output_dim=output_vocab_size,\n",
        "    dropout_p=0.5\n",
        ")\n",
        "model.to(device)\n",
        "print(f\"Model expected output size: {model.fc2.out_features}\")\n",
        "\n",
        "# Optimizer and Learning Rate Scheduler\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n",
        "onecycle_scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "                                             steps_per_epoch=len(train_dataloader),\n",
        "                                             max_lr=0.005,\n",
        "                                             epochs=epochs)"
      ],
      "metadata": {
        "id": "jMld5cKw6LVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "up8KG5BK9da0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unk_answer_idx = answer_vocab['<UNK>']\n",
        "criterion = torch.nn.NLLLoss(ignore_index=unk_answer_idx).to(device)\n",
        "\n",
        "min_loss = float('inf')\n",
        "\n",
        "train_losses_lstm = []\n",
        "train_accs_lstm = []\n",
        "val_losses_lstm = []\n",
        "val_accs_lstm = []\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/vqa_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "best_model_path = os.path.join(save_dir, \"best_model_lstm.pth\")\n",
        "\n",
        "print(f\"Starting Training Loop, saving best model to: {best_model_path}\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start = dt.now()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, data in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "        images = data['image'].to(device)\n",
        "        questions = data['question'].to(device)\n",
        "        answers = data['answer']\n",
        "        multi_answers = data['multi_answer']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images, questions)\n",
        "        predictions = torch.argmax(output.cpu(), dim=1)\n",
        "\n",
        "        indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "        for idx in indices_incorrect:\n",
        "            if predictions[idx] in multi_answers[idx]:\n",
        "                answers[idx] = multi_answers[idx][torch.where(multi_answers[idx] == predictions[idx])[0].item()]\n",
        "\n",
        "        answers_gpu = answers.to(device)\n",
        "        loss = criterion(torch.log(output + 1e-7), answers_gpu)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        onecycle_scheduler.step()\n",
        "\n",
        "        predictions[predictions == unk_answer_idx] = -1\n",
        "        correct = (predictions == answers).sum().item()\n",
        "\n",
        "        train_correct += correct\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_train_acc = train_correct / len(train_dataloader.dataset)\n",
        "    train_losses_lstm.append(avg_train_loss)\n",
        "    train_accs_lstm.append(avg_train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    # lists cm\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\")):\n",
        "            images = data['image'].to(device)\n",
        "            questions = data['question'].to(device)\n",
        "            answers = data['answer']\n",
        "            multi_answers = data['multi_answer']\n",
        "\n",
        "            output = model(images, questions)\n",
        "            vloss = criterion(torch.log(output + 1e-7), answers.to(device))\n",
        "            val_loss += vloss.item()\n",
        "\n",
        "            predictions = torch.argmax(output.cpu(), 1)\n",
        "\n",
        "            indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "            for idx in indices_incorrect:\n",
        "                if predictions[idx] in multi_answers[idx]:\n",
        "                    answers[idx] = multi_answers[idx][torch.where(multi_answers[idx] == predictions[idx])[0].item()]\n",
        "\n",
        "            predictions[predictions == unk_answer_idx] = -1\n",
        "            correct = (predictions == answers).sum().item()\n",
        "            val_correct += correct\n",
        "\n",
        "            # labels for confusion matrix\n",
        "            all_true.extend(answers.tolist())\n",
        "            all_pred.extend(predictions.tolist())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    avg_val_acc = val_correct / len(val_dataloader.dataset)\n",
        "    val_losses_lstm.append(avg_val_loss)\n",
        "    val_accs_lstm.append(avg_val_acc)\n",
        "\n",
        "    epoch_duration = dt.now() - start\n",
        "\n",
        "    print(f'\\nEpoch {epoch+1} Summary => '\n",
        "          f'Train Loss: {avg_train_loss:.5f} | Train Acc: {avg_train_acc:.4f} | '\n",
        "          f'Val Loss: {avg_val_loss:.5f} | Val Acc: {avg_val_acc:.4f} [{epoch_duration}]')\n",
        "\n",
        "    # save best model based on validation loss\n",
        "    if avg_val_loss < min_loss:\n",
        "        print(f\"Validation loss improved ({min_loss:.5f} -> {avg_val_loss:.5f}). Saving model...\")\n",
        "        min_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(f\"Training finished. Best model saved at: {best_model_path}\")"
      ],
      "metadata": {
        "id": "70duQwH-rYun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot**"
      ],
      "metadata": {
        "id": "3yupQCqJ9hcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = min(len(train_losses_lstm), len(val_losses_lstm),\n",
        "              len(train_accs_lstm), len(val_accs_lstm))\n",
        "\n",
        "epochs = range(1, min_len + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses_lstm[:min_len], 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_losses_lstm[:min_len], 'r-', label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accs_lstm[:min_len], 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accs_lstm[:min_len], 'r-', label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HH1BlujbzPe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mapping\n",
        "idx2word = {v: k for k, v in answer_vocab.items()}\n",
        "\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "\n",
        "# filter\n",
        "mask = (all_pred != -1)\n",
        "all_true_filtered = all_true[mask]\n",
        "all_pred_filtered = all_pred[mask]\n",
        "\n",
        "# find top 10\n",
        "true_label_counts = Counter(all_true_filtered)\n",
        "top_label_indices = [label for label, _ in true_label_counts.most_common(10)]\n",
        "\n",
        "# cm\n",
        "cm = confusion_matrix(all_true_filtered, all_pred_filtered, labels=top_label_indices)\n",
        "\n",
        "# convert to strings\n",
        "def idx_to_word(idx):\n",
        "    return idx2word.get(idx, str(idx))\n",
        "\n",
        "disp_labels = [idx_to_word(idx) for idx in top_label_indices]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=disp_labels, yticklabels=disp_labels, cmap='Blues')\n",
        "plt.xlabel('Predicted Answers')\n",
        "plt.ylabel('True Answers')\n",
        "plt.title('Confusion Matrix (Top 10 Answers)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XQfgp3ZMP3U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM with attention**"
      ],
      "metadata": {
        "id": "NQyIKco4ioek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Mechanism\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, img_dim = 1024, glimpses = 2):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.glimpses  = glimpses\n",
        "        self.img_conv = nn.Conv2d(img_dim, 1024, 1)\n",
        "        self.glimpse_conv = nn.Conv2d(1024, glimpses, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, i_feats, q_feats):\n",
        "\n",
        "        # adapt i_feats to be 4D (N, C, H, W)\n",
        "        if len(i_feats.shape) == 2:\n",
        "            # flattened features\n",
        "            i_feats_reshaped = i_feats.unsqueeze(-1).unsqueeze(-1)\n",
        "        else:\n",
        "            i_feats_reshaped = i_feats # 4D (batch, C, H, W)\n",
        "\n",
        "        # reshape\n",
        "        i = F.gelu(self.img_conv(i_feats_reshaped))\n",
        "\n",
        "        # unsqueeze and expand for broadcasting\n",
        "        q = q_feats.unsqueeze(-1).unsqueeze(-1).expand_as(i)\n",
        "\n",
        "        # adding i and q features\n",
        "        combined = F.gelu(i + q)\n",
        "\n",
        "        # getting attention features\n",
        "        glimpse_conv = self.dropout(F.gelu(self.glimpse_conv(combined)))\n",
        "\n",
        "        # glimpse\n",
        "        glimpse_conv = glimpse_conv.view(i_feats.size(0), self.glimpses, -1)\n",
        "\n",
        "        # softmax across 2 features\n",
        "        attention = F.softmax(glimpse_conv, dim=-1).unsqueeze(2)\n",
        "\n",
        "        # unsqueeze and flat\n",
        "        i_feats_flattened = i.view(i.size(0), 1, i.size(1), -1)\n",
        "\n",
        "        # getting attention weights\n",
        "        weighted = attention * i_feats_flattened\n",
        "\n",
        "        # transform\n",
        "        weighted_mean = weighted.sum(dim=-1).view(i_feats.size(0), -1)\n",
        "\n",
        "        return weighted_mean\n",
        "\n",
        "\n",
        "class LSTM_Att_bert(nn.Module):\n",
        "    def __init__(self, img_dim, output_dim, dropout_p = 0.5): # output_vocab_size is implicitly 1000 top answers\n",
        "        super(LSTM_Att_bert, self).__init__()\n",
        "\n",
        "        # layers\n",
        "        bert_model_instance = BertModel.from_pretrained('bert-base-uncased')\n",
        "        bert_embedding_dim = bert_model_instance.embeddings.word_embeddings.weight.shape[1]\n",
        "        bert_vocab_size = bert_model_instance.embeddings.word_embeddings.weight.shape[0]\n",
        "\n",
        "        # bert\n",
        "        self.embedding = nn.Embedding(bert_vocab_size, bert_embedding_dim, padding_idx = 0).from_pretrained(bert_model_instance.embeddings.word_embeddings.weight, freeze = True)\n",
        "        self.fc_embed = nn.Linear(bert_embedding_dim, bert_embedding_dim)\n",
        "\n",
        "        # lstm takes bert embeddings\n",
        "        self.lstm = nn.LSTM(bert_embedding_dim, 1024, num_layers =2, bidirectional = True, batch_first = True)\n",
        "\n",
        "        # fc\n",
        "        self.fc_cell = nn.Linear(1024, 1024)\n",
        "\n",
        "        # attention\n",
        "        self.attention = Attention(img_dim)\n",
        "\n",
        "        # fc layers\n",
        "        self.fc1 = nn.Linear(2048+1024, 1024)\n",
        "        self.fc2 = nn.Linear(1024, output_dim) # dynamic output size\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # initializing weights\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def forward(self, img_input, text_input):\n",
        "\n",
        "        # gelu non-linearity\n",
        "        text_embedded = F.gelu(self.fc_embed(self.embedding(text_input)))\n",
        "\n",
        "        # lstm_cells\n",
        "        _, (_, lstm_cell)  = self.lstm(text_embedded)\n",
        "\n",
        "        # question features\n",
        "        question_feats = self.dropout(F.gelu(self.fc_cell(lstm_cell[-1])))\n",
        "\n",
        "        # L2-normalized img feats\n",
        "        img_feats = F.normalize(img_input, dim = 1, p=2)\n",
        "\n",
        "        # getting attention weights\n",
        "        weighted_img_attention = self.attention(img_feats, question_feats)\n",
        "\n",
        "        # combine\n",
        "        combined = torch.cat([weighted_img_attention, question_feats], dim = 1)\n",
        "\n",
        "        # fc\n",
        "        fc_1 = self.dropout(F.gelu(self.fc1(combined)))\n",
        "\n",
        "        # output\n",
        "        output = F.softmax(self.fc2(fc_1), dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def init_weights(self):\n",
        "        # kaiming_uniform\n",
        "        init = torch.nn.init.kaiming_uniform_\n",
        "        for layer in self.lstm.all_weights:\n",
        "            for hidden in layer[:2]:\n",
        "                init(hidden)\n",
        "\n",
        "        init(self.fc_embed.weight)\n",
        "        init(self.attention.img_conv.weight)\n",
        "        init(self.attention.glimpse_conv.weight)\n",
        "        init(self.fc_cell.weight)\n",
        "        init(self.fc1.weight)\n",
        "        init(self.fc2.weight)"
      ],
      "metadata": {
        "id": "liibbCdU4JM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare for training**"
      ],
      "metadata": {
        "id": "igQhi3j89zfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 2\n",
        "batch_size = 512\n",
        "epochs = 15\n",
        "\n",
        "# Data Loading\n",
        "train_list, val_list = load_split_lists(full_path)\n",
        "\n",
        "train_img_ids, train_features_np = load_features_by_split(\"train2014\", full_path)\n",
        "val_img_ids, val_features_np = load_features_by_split(\"val2014\", full_path)\n",
        "\n",
        "train_features = torch.tensor(train_features_np, dtype=torch.float32)\n",
        "val_features = torch.tensor(val_features_np, dtype=torch.float32)\n",
        "\n",
        "actual_img_dim = train_features.shape[1]\n",
        "print(f\"Detected image feature dimension (img_dim): {actual_img_dim}\")\n",
        "\n",
        "# create vocabs\n",
        "question_vocab = create_question_vocab(train_list)\n",
        "answer_vocab = create_answer_vocab(train_list, top=1000) # tok 1000 answers\n",
        "output_vocab_size = len(answer_vocab)\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = VQADataset(json_list=train_list,\n",
        "                             question_vocab=question_vocab, # question_vocab\n",
        "                             answer_vocab=answer_vocab,\n",
        "                             split='train',\n",
        "                             train_img_feats=(train_img_ids, train_features))\n",
        "\n",
        "val_dataset = VQADataset(json_list=val_list,\n",
        "                            question_vocab=question_vocab, # question_vocab\n",
        "                            answer_vocab=answer_vocab,\n",
        "                            split='val',\n",
        "                            val_img_feats=(val_img_ids, val_features))\n",
        "\n",
        "# Clear lists and numpy arrays to free up memory after dataset creation\n",
        "del train_list\n",
        "del val_list\n",
        "del train_features_np\n",
        "del val_features_np\n",
        "del train_features\n",
        "del val_features\n",
        "print(\"Data loaded and datasets initialized. Memory for raw data cleared.\")\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=num_workers,\n",
        "                                  pin_memory=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=num_workers,\n",
        "                            pin_memory=True)\n",
        "\n",
        "print(\"DataLoaders Initialized\")"
      ],
      "metadata": {
        "id": "eQdfk5-h45fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model Initialization\n",
        "model = LSTM_Att_bert(img_dim=actual_img_dim, output_dim=output_vocab_size, dropout_p=0.5)\n",
        "model.to(device)\n",
        "print(f\"Model initialized and moved to {device}.\")\n",
        "print(f\"Model expected output size: {model.fc2.out_features}\")\n",
        "\n",
        "# Optimizer and Learning Rate Scheduler\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n",
        "onecycle_scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "                                             steps_per_epoch=len(train_dataloader),\n",
        "                                             max_lr=0.005,\n",
        "                                             epochs=epochs)\n"
      ],
      "metadata": {
        "id": "Tn1ofl-tinzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "cSqA7hkX93cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unk_answer_idx = answer_vocab['<UNK>']\n",
        "criterion = torch.nn.NLLLoss(ignore_index=unk_answer_idx).to(device)\n",
        "\n",
        "min_loss = float('inf')\n",
        "\n",
        "train_losses_lstm_att = []\n",
        "train_accs_lstm_att = []\n",
        "val_losses_lstm_att = []\n",
        "val_accs_lstm_att = []\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/vqa_models\"\n",
        "best_model_path = os.path.join(save_dir, \"best_model_lstm_attention.pth\")\n",
        "\n",
        "print(f\"\\n--- Starting Training Loop for LSTM with Attention ---\")\n",
        "print(f\"Saving best model to: {best_model_path}\")\n",
        "cell_start = dt.now()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start = dt.now()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, data in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "        images = data['image'].to(device)\n",
        "        questions = data['question'].to(device)\n",
        "        answers = data['answer']\n",
        "        multi_answers = data['multi_answer']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images, questions)\n",
        "        predictions = torch.argmax(output.cpu(), dim=1)\n",
        "\n",
        "        indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "        for idx in indices_incorrect:\n",
        "            if predictions[idx] in multi_answers[idx]:\n",
        "                answers[idx] = multi_answers[idx][torch.where(multi_answers[idx] == predictions[idx])[0].item()]\n",
        "\n",
        "        answers_gpu = answers.to(device)\n",
        "        loss = criterion(torch.log(output + 1e-7), answers_gpu)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        onecycle_scheduler.step()\n",
        "\n",
        "        predictions[predictions == unk_answer_idx] = -1\n",
        "        correct = (predictions == answers).sum().item()\n",
        "\n",
        "        train_correct += correct\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_train_acc = train_correct / len(train_dataloader.dataset)\n",
        "    train_losses_lstm_att.append(avg_train_loss)\n",
        "    train_accs_lstm_att.append(avg_train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    # lists cm\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\")):\n",
        "            images = data['image'].to(device)\n",
        "            questions = data['question'].to(device)\n",
        "            answers = data['answer']\n",
        "            multi_answers = data['multi_answer']\n",
        "\n",
        "            output = model(images, questions)\n",
        "            vloss = criterion(torch.log(output + 1e-7), answers.to(device))\n",
        "            val_loss += vloss.item()\n",
        "\n",
        "            predictions = torch.argmax(output.cpu(), dim=1)\n",
        "\n",
        "            indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "            for idx in indices_incorrect:\n",
        "                if predictions[idx] in multi_answers[idx]:\n",
        "                    answers[idx] = multi_answers[idx][torch.where(multi_answers[idx] == predictions[idx])[0].item()]\n",
        "\n",
        "            predictions[predictions == unk_answer_idx] = -1\n",
        "            correct = (predictions == answers).sum().item()\n",
        "            val_correct += correct\n",
        "\n",
        "            # collect labels for confusion matrix\n",
        "            all_true.extend(answers.tolist())\n",
        "            all_pred.extend(predictions.tolist())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    avg_val_acc = val_correct / len(val_dataloader.dataset)\n",
        "    val_losses_lstm_att.append(avg_val_loss)\n",
        "    val_accs_lstm_att.append(avg_val_acc)\n",
        "\n",
        "    if avg_val_loss < min_loss:\n",
        "        print(f\"Validation loss improved from {min_loss:.5f} to {avg_val_loss:.5f}. Saving model...\")\n",
        "        min_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "    epoch_duration = dt.now() - start\n",
        "    print(f'\\nEpoch {epoch+1} Summary => '\n",
        "          f'Train Loss: {avg_train_loss:.5f} | Train Acc: {avg_train_acc:.4f} | '\n",
        "          f'Val Loss: {avg_val_loss:.5f} | Val Acc: {avg_val_acc:.4f} [{epoch_duration}]')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Training finished. Best model saved at: {best_model_path}\")"
      ],
      "metadata": {
        "id": "TSXElK1e581q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot**"
      ],
      "metadata": {
        "id": "0VJJcA4V975b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = min(len(train_losses_lstm_att), len(val_losses_lstm_att),\n",
        "              len(train_accs_lstm_att), len(val_accs_lstm_att))\n",
        "\n",
        "epochs = range(1, min_len + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses_lstm_att[:min_len], 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_losses_lstm_att[:min_len], 'r-', label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accs_lstm_att[:min_len], 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accs_lstm_att[:min_len], 'r-', label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pZIoUPrFAMd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mapping\n",
        "idx2word = {v: k for k, v in answer_vocab.items()}\n",
        "\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "\n",
        "# filter\n",
        "mask = (all_pred != -1)\n",
        "all_true_filtered = all_true[mask]\n",
        "all_pred_filtered = all_pred[mask]\n",
        "\n",
        "# find top 10\n",
        "true_label_counts = Counter(all_true_filtered)\n",
        "top_label_indices = [label for label, _ in true_label_counts.most_common(10)]\n",
        "\n",
        "# cm\n",
        "cm = confusion_matrix(all_true_filtered, all_pred_filtered, labels=top_label_indices)\n",
        "\n",
        "# convert to strings\n",
        "def idx_to_word(idx):\n",
        "    return idx2word.get(idx, str(idx))\n",
        "\n",
        "disp_labels = [idx_to_word(idx) for idx in top_label_indices]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=disp_labels, yticklabels=disp_labels, cmap='Blues')\n",
        "plt.xlabel('Predicted Answers')\n",
        "plt.ylabel('True Answers')\n",
        "plt.title('Confusion Matrix (Top 10 Answers)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aydZpMvJRY_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1-D CNN**"
      ],
      "metadata": {
        "id": "iKmSPWNJHgn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN1D model\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, vocab_size, img_dim, embedding_dim=300, num_filters=512, kernel_sizes=[3, 4, 5], output_dim=1000, dropout_p=0.5):\n",
        "        super(CNN1D, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=question_vocab['<PAD>'])\n",
        "\n",
        "        # nn module\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_filters,\n",
        "                      kernel_size=k) for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        # calculate the total output dimension\n",
        "        self.cnn_output_dim = num_filters * len(kernel_sizes)\n",
        "\n",
        "        # layers for question embeddings\n",
        "        self.fc_question = nn.Linear(self.cnn_output_dim, 1024)\n",
        "\n",
        "        # layers for image embedding\n",
        "        self.fc_image = nn.Linear(img_dim, 1024)\n",
        "\n",
        "        # fusion and classification layers\n",
        "        self.fc1 = nn.Linear(1024, 1000)\n",
        "        self.fc2 = nn.Linear(1000, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, img_feat_input, text_input):\n",
        "        # embedded input\n",
        "        embedded = self.embedding(text_input)\n",
        "\n",
        "        # chage positions\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        # apply 1D CNNs and max pooling\n",
        "        cnn_outputs = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled_outputs = [F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2) for conv_out in cnn_outputs]\n",
        "\n",
        "        # concatenate outputs\n",
        "        question_features = torch.cat(pooled_outputs, 1)\n",
        "\n",
        "        # process question features\n",
        "        fc_question = self.dropout(F.gelu(self.fc_question(question_features)))\n",
        "\n",
        "        # process image embedding\n",
        "        fc_image = F.gelu(self.fc_image(F.normalize(img_feat_input, dim=1, p=2)))\n",
        "\n",
        "        # point-wise multiplication\n",
        "        pw_mul = fc_image * fc_question\n",
        "\n",
        "        # classification layers\n",
        "        fc_1 = self.dropout(F.gelu(self.fc1(self.dropout(pw_mul))))\n",
        "        output = F.softmax(self.fc2(fc_1), dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def init_weights(self):\n",
        "      # Kaiming uniform initialization\n",
        "      init = nn.init.kaiming_uniform_\n",
        "\n",
        "      # initialize uniformly\n",
        "      nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
        "\n",
        "      # Initialize conv1d weights and biases\n",
        "      for conv in self.convs:\n",
        "          init(conv.weight)\n",
        "          if conv.bias is not None:\n",
        "              nn.init.zeros_(conv.bias)\n",
        "\n",
        "      # Initialize fully connected layers weights and biases\n",
        "      fc_layers = [self.fc_question, self.fc_image, self.fc1, self.fc2]\n",
        "      for layer in fc_layers:\n",
        "          init(layer.weight)\n",
        "          if layer.bias is not None:\n",
        "              nn.init.zeros_(layer.bias)"
      ],
      "metadata": {
        "id": "BZZndvwKF0Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare for training**"
      ],
      "metadata": {
        "id": "m9XxG3G--MT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 2\n",
        "batch_size = 512\n",
        "epochs = 15\n",
        "max_question_length = 25 # 25 words for a question\n",
        "\n",
        "# Data Loading\n",
        "train_list, val_list = load_split_lists(full_path)\n",
        "\n",
        "train_img_ids, train_features_np = load_features_by_split(\"train2014\", full_path)\n",
        "val_img_ids, val_features_np = load_features_by_split(\"val2014\", full_path)\n",
        "\n",
        "train_features = torch.tensor(train_features_np, dtype=torch.float32)\n",
        "val_features = torch.tensor(val_features_np, dtype=torch.float32)\n",
        "\n",
        "actual_img_dim = train_features.shape[1]\n",
        "print(f\"Detected image feature dimension (img_dim): {actual_img_dim}\")\n",
        "\n",
        "# Create Vocabularies\n",
        "question_vocab = create_question_vocab(train_list)\n",
        "answer_vocab = create_answer_vocab(train_list)\n",
        "\n",
        "model_output_size = len(answer_vocab)\n",
        "print(f\"Custom question vocabulary size: {len(question_vocab)}\")\n",
        "print(f\"Answer vocabulary size: {len(answer_vocab)}\")\n",
        "print(f\"Model's final output layer size: {model_output_size}\")\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = VQADataset(json_list=train_list,\n",
        "                             question_vocab=question_vocab,\n",
        "                             answer_vocab=answer_vocab,\n",
        "                             split='train',\n",
        "                             train_img_feats=(train_img_ids, train_features),\n",
        "                             max_question_length=max_question_length)\n",
        "\n",
        "val_dataset = VQADataset(json_list=val_list,\n",
        "                            question_vocab=question_vocab,\n",
        "                            answer_vocab=answer_vocab,\n",
        "                            split='val',\n",
        "                            val_img_feats=(val_img_ids, val_features),\n",
        "                            max_question_length=max_question_length)\n",
        "\n",
        "# Clear lists and numpy arrays to free up memory after dataset creation\n",
        "del train_list\n",
        "del val_list\n",
        "del train_features_np\n",
        "del val_features_np\n",
        "del train_features\n",
        "del val_features\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=num_workers,\n",
        "                            pin_memory=True)"
      ],
      "metadata": {
        "id": "aQGrhjhLGGyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model Initialization\n",
        "model = CNN1D(vocab_size=len(question_vocab),\n",
        "                       img_dim=actual_img_dim,\n",
        "                       embedding_dim=300, # the dimention of the embeddings 300 suitable for resnet\n",
        "                       num_filters=512,\n",
        "                       kernel_sizes=[3, 4, 5],\n",
        "                       output_dim=model_output_size,\n",
        "                       dropout_p=0.5)\n",
        "model.to(device)\n",
        "print(f\"Model initialized and moved to {device}.\")\n",
        "print(f\"Model expected output size: {model.fc2.out_features}\")\n",
        "\n",
        "# Optimizer and Learning Rate Scheduler\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n",
        "onecycle_scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "                                             steps_per_epoch=len(train_dataloader),\n",
        "                                             max_lr=0.005,\n",
        "                                             epochs=epochs)\n",
        "\n",
        "# Negative Log Likelihood Loss\n",
        "unk_answer_idx = answer_vocab['<UNK>']\n",
        "criterion = torch.nn.NLLLoss(ignore_index=unk_answer_idx).to(device)"
      ],
      "metadata": {
        "id": "KvFtkImkGnpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "dEQ0gX4F-Rie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_cnn = []\n",
        "train_accs_cnn = []\n",
        "val_losses_cnn = []\n",
        "val_accs_cnn = []\n",
        "\n",
        "min_loss = float('inf')  # track best validation loss\n",
        "\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/vqa_models\"\n",
        "best_model_path = os.path.join(save_dir, \"best_model_1dCnn.pth\")\n",
        "\n",
        "print(\"\\n--- Starting Training Loop ---\")\n",
        "cell_start = dt.now()  # Total time counter\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start = dt.now()  # Epoch timer\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, data in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "        images = data['image'].to(device)\n",
        "        questions = data['question'].to(device)\n",
        "        answers = data['answer']\n",
        "        multi_answers = data['multi_answer']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images, questions)\n",
        "        predictions = torch.argmax(output.cpu(), dim=1)\n",
        "\n",
        "        # handle alternative answers\n",
        "        indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "        for idx in indices_incorrect:\n",
        "            if predictions[idx] in multi_answers[idx]:\n",
        "                match_idx = torch.where(multi_answers[idx] == predictions[idx])[0].item()\n",
        "                answers[idx] = multi_answers[idx][match_idx]\n",
        "\n",
        "        answers_gpu = answers.to(device)\n",
        "        loss = criterion(torch.log(output + 1e-7), answers_gpu)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        onecycle_scheduler.step()\n",
        "\n",
        "        predictions_for_acc = predictions.clone()\n",
        "        predictions_for_acc[predictions_for_acc == unk_answer_idx] = -1\n",
        "        correct = (predictions_for_acc == answers).sum().item()\n",
        "\n",
        "        train_correct += correct\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_train_acc = train_correct / len(train_dataloader.dataset)\n",
        "    train_losses_cnn.append(avg_train_loss)\n",
        "    train_accs_cnn.append(avg_train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    # cm lists\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\")):\n",
        "            images = data['image'].to(device)\n",
        "            questions = data['question'].to(device)\n",
        "            answers = data['answer']\n",
        "            multi_answers = data['multi_answer']\n",
        "\n",
        "            output = model(images, questions)\n",
        "            vloss = criterion(torch.log(output + 1e-7), answers.to(device))\n",
        "            val_loss += vloss.item()\n",
        "\n",
        "            predictions = torch.argmax(output.cpu(), 1)\n",
        "\n",
        "            indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "            for idx in indices_incorrect:\n",
        "                if predictions[idx] in multi_answers[idx]:\n",
        "                    match_idx = torch.where(multi_answers[idx] == predictions[idx])[0].item()\n",
        "                    answers[idx] = multi_answers[idx][match_idx]\n",
        "\n",
        "            predictions_for_acc = predictions.clone()\n",
        "            predictions_for_acc[predictions_for_acc == unk_answer_idx] = -1\n",
        "            correct = (predictions_for_acc == answers).sum().item()\n",
        "            val_correct += correct\n",
        "\n",
        "            # collect labels for confusion matrix\n",
        "            all_true.extend(answers.tolist())\n",
        "            all_pred.extend(predictions.tolist())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    avg_val_acc = val_correct / len(val_dataloader.dataset)\n",
        "    val_losses_cnn.append(avg_val_loss)\n",
        "    val_accs_cnn.append(avg_val_acc)\n",
        "\n",
        "    epoch_duration = dt.now() - start\n",
        "    print(f'\\nEpoch {epoch+1} Summary => '\n",
        "          f'Train Loss: {avg_train_loss:.5f} | Train Acc: {avg_train_acc:.4f} | '\n",
        "          f'Val Loss: {avg_val_loss:.5f} | Val Acc: {avg_val_acc:.4f} [{epoch_duration}]')\n",
        "\n",
        "    # Save best model  based on validation loss\n",
        "    if avg_val_loss < min_loss:\n",
        "        print(f\"Validation loss improved ({min_loss:.5f} -> {avg_val_loss:.5f}). Saving model...\")\n",
        "        min_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "total_duration = dt.now() - cell_start\n",
        "print(f\"\\n--- Training Finished! Total time: {total_duration} ---\")"
      ],
      "metadata": {
        "id": "m0ooD5mlHf5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot**"
      ],
      "metadata": {
        "id": "QmQAgtUR-U-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = min(len(train_losses_cnn), len(val_losses_cnn),\n",
        "              len(train_accs_cnn), len(val_accs_cnn))\n",
        "\n",
        "epochs = range(1, min_len + 1)\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(epochs, train_losses_cnn[:min_len], 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_losses_cnn[:min_len], 'r-', label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cnn_loss_plot.png')  # Save as image\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(epochs, train_accs_cnn[:min_len], 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accs_cnn[:min_len], 'r-', label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cnn_accuracy_plot.png')  # Save as image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b0E2XYKOGsep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mapping\n",
        "idx2word = {v: k for k, v in answer_vocab.items()}\n",
        "\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "\n",
        "# filter\n",
        "mask = (all_pred != -1)\n",
        "all_true_filtered = all_true[mask]\n",
        "all_pred_filtered = all_pred[mask]\n",
        "\n",
        "# find top 10\n",
        "true_label_counts = Counter(all_true_filtered)\n",
        "top_label_indices = [label for label, _ in true_label_counts.most_common(10)]\n",
        "\n",
        "# cm\n",
        "cm = confusion_matrix(all_true_filtered, all_pred_filtered, labels=top_label_indices)\n",
        "\n",
        "# convert to strings\n",
        "def idx_to_word(idx):\n",
        "    return idx2word.get(idx, str(idx))\n",
        "\n",
        "disp_labels = [idx_to_word(idx) for idx in top_label_indices]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=disp_labels, yticklabels=disp_labels, cmap='Blues')\n",
        "plt.xlabel('Predicted Answers')\n",
        "plt.ylabel('True Answers')\n",
        "plt.title('Confusion Matrix (Top 10 Answers)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('cnn_confusion_matrix.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kMkWCRiARt1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RNN**"
      ],
      "metadata": {
        "id": "NtCMWiJXKBcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, img_dim, embedding_dim=300, rnn_hidden_size=512, num_rnn_layers=2, output_dim=1000, dropout_p=0.5):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=question_vocab['<PAD>'])\n",
        "        self.fc_embed = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim,\n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          num_layers=num_rnn_layers,\n",
        "                          batch_first=True,\n",
        "                          dropout=dropout_p if num_rnn_layers > 1 else 0) # Dropout only if > 1 layer\n",
        "\n",
        "        # The concatenated RNN\n",
        "        self.fc_rnn = nn.Linear(num_rnn_layers * rnn_hidden_size, 1024)\n",
        "\n",
        "        # Layers for image embedding\n",
        "        self.fc_image = nn.Linear(img_dim, 1024)\n",
        "\n",
        "        # Fusion and classification layers\n",
        "        self.fc1 = nn.Linear(1024, 1000)\n",
        "        self.fc2 = nn.Linear(1000, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, img_feat_input, text_input):\n",
        "        # Text Embedding\n",
        "        embedded = F.gelu(self.fc_embed(self.embedding(text_input)))\n",
        "\n",
        "        # apply RNN\n",
        "        _, h_n = self.rnn(embedded)\n",
        "\n",
        "        # concatenate hidden states from all layers for question representation\n",
        "        rnn_concat = h_n.permute(1, 0, 2).reshape(h_n.size(1), -1)\n",
        "\n",
        "        # process question features\n",
        "        fc_question = self.dropout(F.gelu(self.fc_rnn(rnn_concat)))\n",
        "\n",
        "        # process image embedding\n",
        "        fc_image = F.gelu(self.fc_image(F.normalize(img_feat_input, dim=1, p=2)))\n",
        "\n",
        "        # point-wise multiplication\n",
        "        pw_mul = fc_image * fc_question\n",
        "\n",
        "        # classification layers\n",
        "        fc_1 = self.dropout(F.gelu(self.fc1(self.dropout(pw_mul))))\n",
        "        output = F.softmax(self.fc2(fc_1), dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def init_weights(self):\n",
        "            init = nn.init.kaiming_uniform_\n",
        "\n",
        "            # Initialize embedding and linear layers\n",
        "            init(self.embedding.weight)\n",
        "            init(self.fc_embed.weight)\n",
        "            init(self.fc_rnn.weight)\n",
        "            init(self.fc_image.weight)\n",
        "            init(self.fc1.weight)\n",
        "            init(self.fc2.weight)\n",
        "\n",
        "            # Initialize RNN weights\n",
        "            for name, param in self.rnn.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    init(param)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(param, 0)"
      ],
      "metadata": {
        "id": "fWRzp-hqHIHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare for training**"
      ],
      "metadata": {
        "id": "vkzTPmVH-fQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 2\n",
        "batch_size = 512\n",
        "epochs = 15\n",
        "max_question_length = 25\n",
        "\n",
        "# Data Loading\n",
        "train_list, val_list = load_split_lists(full_path)\n",
        "\n",
        "train_img_ids, train_features_np = load_features_by_split(\"train2014\", full_path)\n",
        "val_img_ids, val_features_np = load_features_by_split(\"val2014\", full_path)\n",
        "\n",
        "train_features = torch.tensor(train_features_np, dtype=torch.float32)\n",
        "val_features = torch.tensor(val_features_np, dtype=torch.float32)\n",
        "\n",
        "actual_img_dim = train_features.shape[1]\n",
        "print(f\"Detected image feature dimension (img_dim): {actual_img_dim}\")\n",
        "\n",
        "# Create Vocabularies\n",
        "question_vocab = create_question_vocab(train_list)\n",
        "answer_vocab = create_answer_vocab(train_list)\n",
        "\n",
        "model_output_size = len(answer_vocab)\n",
        "print(f\"Custom question vocabulary size: {len(question_vocab)}\")\n",
        "print(f\"Answer vocabulary size: {len(answer_vocab)}\")\n",
        "print(f\"Model's final output layer size: {model_output_size}\")\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = VQADataset(json_list=train_list,\n",
        "                             question_vocab=question_vocab,\n",
        "                             answer_vocab=answer_vocab,\n",
        "                             split='train',\n",
        "                             train_img_feats=(train_img_ids, train_features),\n",
        "                             max_question_length=max_question_length)\n",
        "\n",
        "val_dataset = VQADataset(json_list=val_list,\n",
        "                            question_vocab=question_vocab,\n",
        "                            answer_vocab=answer_vocab,\n",
        "                            split='val',\n",
        "                            val_img_feats=(val_img_ids, val_features),\n",
        "                            max_question_length=max_question_length)\n",
        "\n",
        "# Clear lists and numpy arrays to free up memory after dataset creation\n",
        "del train_list\n",
        "del val_list\n",
        "del train_features_np\n",
        "del val_features_np\n",
        "del train_features\n",
        "del val_features\n",
        "print(\"Data loaded and datasets initialized. Memory for raw data cleared.\")\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=num_workers,\n",
        "                            pin_memory=True)"
      ],
      "metadata": {
        "id": "HQ9O-RWiHNX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model Initialisation\n",
        "model = RNN(vocab_size=len(question_vocab),\n",
        "                     img_dim=actual_img_dim,\n",
        "                     embedding_dim=300, # suitable for resnet\n",
        "                     rnn_hidden_size=512, # Hidden size per RNN layer\n",
        "                     num_rnn_layers=2, # Number of stacked RNN layers\n",
        "                     output_dim=model_output_size,\n",
        "                     dropout_p=0.5)\n",
        "model.to(device)\n",
        "print(f\"Model initialized and moved to {device}.\")\n",
        "print(f\"Model expected output size: {model.fc2.out_features}\")\n",
        "\n",
        "# Optimizer and Learning Rate Scheduler\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n",
        "onecycle_scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "                                             steps_per_epoch=len(train_dataloader),\n",
        "                                             max_lr=0.005,\n",
        "                                             epochs=epochs)\n",
        "\n",
        "# Negative Log Likelihood Loss\n",
        "unk_answer_idx = answer_vocab['<UNK>']\n",
        "criterion = torch.nn.NLLLoss(ignore_index=unk_answer_idx).to(device)\n"
      ],
      "metadata": {
        "id": "peekPz4ZHTcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "oNKxJiVe-juq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_rnn = []\n",
        "train_accs_rnn = []\n",
        "val_losses_rnn = []\n",
        "val_accs_rnn = []\n",
        "\n",
        "min_loss = float('inf')  # track best validation loss\n",
        "\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/vqa_models\"\n",
        "best_model_path = os.path.join(save_dir, \"best_model_RNN.pth\")\n",
        "\n",
        "cell_start = dt.now()  # time counter\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start = dt.now()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, data in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "        images = data['image'].to(device)\n",
        "        questions = data['question'].to(device)\n",
        "        answers = data['answer']\n",
        "        multi_answers = data['multi_answer']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images, questions)\n",
        "        predictions = torch.argmax(output.cpu(), dim=1)\n",
        "\n",
        "        # handle alternative answers\n",
        "        indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "        for idx in indices_incorrect:\n",
        "            if predictions[idx] in multi_answers[idx]:\n",
        "                match_idx = torch.where(multi_answers[idx] == predictions[idx])[0].item()\n",
        "                answers[idx] = multi_answers[idx][match_idx]\n",
        "\n",
        "        answers_gpu = answers.to(device)\n",
        "        loss = criterion(torch.log(output + 1e-7), answers_gpu)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        onecycle_scheduler.step()\n",
        "\n",
        "        predictions_for_acc = predictions.clone()\n",
        "        predictions_for_acc[predictions_for_acc == unk_answer_idx] = -1\n",
        "        correct = (predictions_for_acc == answers).sum().item()\n",
        "\n",
        "        train_correct += correct\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_train_acc = train_correct / len(train_dataloader.dataset)\n",
        "\n",
        "    train_losses_rnn.append(avg_train_loss)\n",
        "    train_accs_rnn.append(avg_train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    # cm lists\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\")):\n",
        "            images = data['image'].to(device)\n",
        "            questions = data['question'].to(device)\n",
        "            answers = data['answer']\n",
        "            multi_answers = data['multi_answer']\n",
        "\n",
        "            output = model(images, questions)\n",
        "            vloss = criterion(torch.log(output + 1e-7), answers.to(device))\n",
        "            val_loss += vloss.item()\n",
        "\n",
        "            predictions = torch.argmax(output.cpu(), 1)\n",
        "\n",
        "            indices_incorrect = torch.where(~(answers == predictions))[0]\n",
        "            for idx in indices_incorrect:\n",
        "                if predictions[idx] in multi_answers[idx]:\n",
        "                    match_idx = torch.where(multi_answers[idx] == predictions[idx])[0].item()\n",
        "                    answers[idx] = multi_answers[idx][match_idx]\n",
        "\n",
        "            predictions_for_acc = predictions.clone()\n",
        "            predictions_for_acc[predictions_for_acc == unk_answer_idx] = -1\n",
        "            correct = (predictions_for_acc == answers).sum().item()\n",
        "            val_correct += correct\n",
        "\n",
        "            # collect labels for confusion matrix\n",
        "            all_true.extend(answers.tolist())\n",
        "            all_pred.extend(predictions.tolist())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    avg_val_acc = val_correct / len(val_dataloader.dataset)\n",
        "\n",
        "    val_losses_rnn.append(avg_val_loss)\n",
        "    val_accs_rnn.append(avg_val_acc)\n",
        "\n",
        "    epoch_duration = dt.now() - start\n",
        "    print(f'\\nEpoch {epoch+1} Summary => '\n",
        "          f'Train Loss: {avg_train_loss:.5f} | Train Acc: {avg_train_acc:.4f} | '\n",
        "          f'Val Loss: {avg_val_loss:.5f} | Val Acc: {avg_val_acc:.4f} [{epoch_duration}]')\n",
        "\n",
        "    # save best model\n",
        "    if avg_val_loss < min_loss:\n",
        "        print(f\"Validation loss improved ({min_loss:.5f} -> {avg_val_loss:.5f}). Saving model...\")\n",
        "        min_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "total_duration = dt.now() - cell_start\n",
        "print(f\"\\n--- Training Finished! Total time: {total_duration} ---\")"
      ],
      "metadata": {
        "id": "jd9IgrvKKAz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot**"
      ],
      "metadata": {
        "id": "hQ_i0cfQ-nih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = min(len(train_losses_rnn), len(val_losses_rnn),\n",
        "              len(train_accs_rnn), len(val_accs_rnn))\n",
        "epochs = range(1, min_len + 1)\n",
        "\n",
        "# Loss Plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(epochs, train_losses_rnn[:min_len], 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_losses_rnn[:min_len], 'r-', label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('rnn_loss_plot.png')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(epochs, train_accs_rnn[:min_len], 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accs_rnn[:min_len], 'r-', label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('rnn_accuracy_plot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e0gJTLhTHYyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mapping\n",
        "idx2word = {v: k for k, v in answer_vocab.items()}\n",
        "\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "\n",
        "# filter\n",
        "mask = (all_pred != -1)\n",
        "all_true_filtered = all_true[mask]\n",
        "all_pred_filtered = all_pred[mask]\n",
        "\n",
        "# find top 10\n",
        "true_label_counts = Counter(all_true_filtered)\n",
        "top_label_indices = [label for label, _ in true_label_counts.most_common(10)]\n",
        "\n",
        "# cm\n",
        "cm = confusion_matrix(all_true_filtered, all_pred_filtered, labels=top_label_indices)\n",
        "\n",
        "# convert to strings\n",
        "def idx_to_word(idx):\n",
        "    return idx2word.get(idx, str(idx))\n",
        "\n",
        "disp_labels = [idx_to_word(idx) for idx in top_label_indices]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=disp_labels, yticklabels=disp_labels, cmap='Blues')\n",
        "plt.xlabel('Predicted Answers')\n",
        "plt.ylabel('True Answers')\n",
        "plt.title('Confusion Matrix (Top 10 Answers)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('rnn_confusion_matrix.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1D53tATKR2HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TEST MODELS**"
      ],
      "metadata": {
        "id": "jkEA6bmlD7Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc http://images.cocodataset.org/zips/val2014.zip\n",
        "!unzip -q val2014.zip\n",
        "!rm *.zip"
      ],
      "metadata": {
        "id": "QiE4jrmzX53-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implement trained ResNet model structure**"
      ],
      "metadata": {
        "id": "H1McZ7iVR5ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, linear_eval=False, unfreeze_layer4=True):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.linear_eval = linear_eval\n",
        "\n",
        "        weights = ResNet50_Weights.DEFAULT\n",
        "        resnet = resnet50(weights=weights)\n",
        "        resnet.fc = Identity()\n",
        "\n",
        "        # freeze all layers\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # unfreeze layer4\n",
        "        if unfreeze_layer4:\n",
        "            for name, param in resnet.named_parameters():\n",
        "                if \"layer4\" in name:\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        self.encoder = resnet\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 256)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.linear_eval:\n",
        "            x = torch.cat(x, dim=0)\n",
        "        features = self.encoder(x)\n",
        "        projection = self.projection(features)\n",
        "        return projection"
      ],
      "metadata": {
        "id": "M37iYC77R4wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LSTM**"
      ],
      "metadata": {
        "id": "wZZGA6-vi7ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = '/content/drive/MyDrive/vqa_models/best_model_lstm.pth'\n",
        "simclr_path = '/content/drive/MyDrive/ResNet50/ResNet50_def.pth'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Vocab and Mapping\n",
        "idx_to_answer = {v: k for k, v in answer_vocab.items()}\n",
        "question_vocab['<PAD>'] = question_vocab.get('<PAD>', 0)\n",
        "\n",
        "# Load VQA Model\n",
        "img_dim = 256\n",
        "vocab_size = len(question_vocab)\n",
        "\n",
        "model = LSTM(vocab_size=vocab_size, img_dim=img_dim, output_dim=1000).to(device)\n",
        "model.load_state_dict(torch.load(full_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load SimCLR Model\n",
        "simclr_model = SimCLR(linear_eval=True).to(device)  # linear_eval=True to avoid augmentation stacking\n",
        "simclr_model.load_state_dict(torch.load(simclr_path, map_location=device))\n",
        "simclr_model.eval()\n",
        "\n",
        "\n",
        "# Image Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Extract Features\n",
        "def extract_image_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Pass tensor directly\n",
        "        features = simclr_model(image_tensor)\n",
        "    return features.squeeze(0)\n",
        "\n",
        "# Encode Question\n",
        "def encode_single_question(question, question_vocab, max_length=25):\n",
        "    tokens = question.lower().strip().split()\n",
        "    token_ids = [question_vocab.get(t, question_vocab.get('<UNK>', 0)) for t in tokens]\n",
        "    token_ids = token_ids[:max_length]\n",
        "    token_ids += [question_vocab['<PAD>']] * (max_length - len(token_ids))\n",
        "    return torch.tensor(token_ids).unsqueeze(0).to(device)\n",
        "\n",
        "# Prediction\n",
        "def predict_topk(image_path, question_text, k=5):\n",
        "    image_tensor = extract_image_features(image_path).unsqueeze(0)\n",
        "    question_tensor = encode_single_question(question_text, question_vocab)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor, question_tensor)\n",
        "        probs = F.softmax(output, dim=1).squeeze(0)\n",
        "\n",
        "        topk_probs, topk_indices = torch.topk(probs, k)\n",
        "        topk_answers = [idx_to_answer.get(i.item(), \"<UNK>\") for i in topk_indices]\n",
        "\n",
        "    print(f\"\\nQuestion: {question_text}\\n\")\n",
        "    print(\"Top Predictions:\")\n",
        "    for i in range(k):\n",
        "        print(f\"{i+1}. {topk_answers[i]} ({topk_probs[i].item()*100:.2f}%)\")\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Q: {question_text}\")\n",
        "    plt.show()\n",
        "\n",
        "# Example Call\n",
        "image_id = 107087\n",
        "image_filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
        "image_path = f\"/content/val2014/{image_filename}\"\n",
        "question = \"Which animal is depicted in the figure?\"\n",
        "\n",
        "predict_topk(image_path, question, k=10)"
      ],
      "metadata": {
        "id": "qke0DthNjKqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LSTM-Attention**"
      ],
      "metadata": {
        "id": "F-vlwMS4i-LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "full_path = '/content/drive/MyDrive/vqa_models/best_model_lstm_attention.pth'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Vocab and Mapping\n",
        "idx_to_answer = {v: k for k, v in answer_vocab.items()}\n",
        "question_vocab['<PAD>'] = question_vocab.get('<PAD>', 0)\n",
        "\n",
        "# Load VQA Model\n",
        "img_dim = 256\n",
        "vocab_size = len(question_vocab)\n",
        "\n",
        "model = LSTM_Att_bert(img_dim=img_dim, output_dim=1000).to(device)\n",
        "model.load_state_dict(torch.load(full_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load SimCLR Model\n",
        "simclr_model = SimCLR(linear_eval=True).to(device)  # linear_eval=True to avoid augmentation stacking\n",
        "simclr_model.load_state_dict(torch.load(simclr_path, map_location=device))\n",
        "simclr_model.eval()\n",
        "\n",
        "# Image Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Extract Features from Image\n",
        "def extract_image_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Pass tensor directly, not wrapped in list\n",
        "        features = simclr_model(image_tensor)\n",
        "    return features.squeeze(0)\n",
        "\n",
        "# Encode Question\n",
        "def encode_single_question(question, question_vocab, max_length=25):\n",
        "    tokens = question.lower().strip().split()\n",
        "    token_ids = [question_vocab.get(t, question_vocab.get('<UNK>', 0)) for t in tokens]\n",
        "    token_ids = token_ids[:max_length]\n",
        "    token_ids += [question_vocab['<PAD>']] * (max_length - len(token_ids))\n",
        "    return torch.tensor(token_ids).unsqueeze(0).to(device)\n",
        "\n",
        "# Prediction\n",
        "def predict_topk(image_path, question_text, k=5):\n",
        "    image_tensor = extract_image_features(image_path).unsqueeze(0)\n",
        "    question_tensor = encode_single_question(question_text, question_vocab)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor, question_tensor)\n",
        "        probs = F.softmax(output, dim=1).squeeze(0)\n",
        "\n",
        "        topk_probs, topk_indices = torch.topk(probs, k)\n",
        "        topk_answers = [idx_to_answer.get(i.item(), \"<UNK>\") for i in topk_indices]\n",
        "\n",
        "    print(f\"\\nQuestion: {question_text}\\n\")\n",
        "    print(\"Top Predictions:\")\n",
        "    for i in range(k):\n",
        "        print(f\"{i+1}. {topk_answers[i]} ({topk_probs[i].item()*100:.2f}%)\")\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Q: {question_text}\")\n",
        "    plt.show()\n",
        "\n",
        "# Example Call\n",
        "image_id = 107087\n",
        "image_filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
        "image_path = f\"/content/val2014/{image_filename}\"\n",
        "question = \"Which animal is depicted in the figure?\"\n",
        "\n",
        "predict_topk(image_path, question, k=10)"
      ],
      "metadata": {
        "id": "rUDbMQvsjYkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1D-CNN**"
      ],
      "metadata": {
        "id": "YHF-zofoi9-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = '/content/drive/MyDrive/vqa_models/best_model_1dCnn.pth'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Vocab and Mapping\n",
        "idx_to_answer = {v: k for k, v in answer_vocab.items()}\n",
        "question_vocab['<PAD>'] = question_vocab.get('<PAD>', 0)\n",
        "\n",
        "# Load VQA Model\n",
        "img_dim = 256\n",
        "vocab_size = len(question_vocab)\n",
        "\n",
        "model = CNN1D(vocab_size=vocab_size, img_dim=img_dim).to(device)\n",
        "model.load_state_dict(torch.load(full_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load SimCLR Model\n",
        "simclr_model = SimCLR(linear_eval=True).to(device)  # linear_eval=True to avoid augmentation stacking\n",
        "simclr_model.load_state_dict(torch.load(simclr_path, map_location=device))\n",
        "simclr_model.eval()\n",
        "\n",
        "# Image Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def extract_image_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Pass tensor directly, not wrapped in list\n",
        "        features = simclr_model(image_tensor)\n",
        "    return features.squeeze(0)\n",
        "\n",
        "# Encode Question\n",
        "def encode_single_question(question, question_vocab, max_length=25):\n",
        "    tokens = question.lower().strip().split()\n",
        "    token_ids = [question_vocab.get(t, question_vocab.get('<UNK>', 0)) for t in tokens]\n",
        "    token_ids = token_ids[:max_length]\n",
        "    token_ids += [question_vocab['<PAD>']] * (max_length - len(token_ids))\n",
        "    return torch.tensor(token_ids).unsqueeze(0).to(device)\n",
        "\n",
        "# Prediction\n",
        "def predict_topk(image_path, question_text, k=5):\n",
        "    image_tensor = extract_image_features(image_path).unsqueeze(0)\n",
        "    question_tensor = encode_single_question(question_text, question_vocab)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor, question_tensor)\n",
        "        probs = F.softmax(output, dim=1).squeeze(0)\n",
        "\n",
        "        topk_probs, topk_indices = torch.topk(probs, k)\n",
        "        topk_answers = [idx_to_answer.get(i.item(), \"<UNK>\") for i in topk_indices]\n",
        "\n",
        "    print(f\"\\nQuestion: {question_text}\\n\")\n",
        "    print(\"Top Predictions:\")\n",
        "    for i in range(k):\n",
        "        print(f\"{i+1}. {topk_answers[i]} ({topk_probs[i].item()*100:.2f}%)\")\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Q: {question_text}\")\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "image_id = 107087\n",
        "image_filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
        "image_path = f\"/content/val2014/{image_filename}\"\n",
        "question = \"Which animal is depicted in the figure?\"\n",
        "\n",
        "predict_topk(image_path, question, k=10)"
      ],
      "metadata": {
        "id": "TblupNv1jZNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RNN**"
      ],
      "metadata": {
        "id": "LKoUu6V5jGOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = '/content/drive/MyDrive/vqa_models/best_model_RNN.pth'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Vocab and Mapping\n",
        "idx_to_answer = {v: k for k, v in answer_vocab.items()}\n",
        "question_vocab['<PAD>'] = question_vocab.get('<PAD>', 0)\n",
        "\n",
        "# Load VQA Model\n",
        "img_dim = 256\n",
        "vocab_size = len(question_vocab)\n",
        "\n",
        "model = RNN(vocab_size=vocab_size, img_dim=img_dim).to(device)\n",
        "model.load_state_dict(torch.load(full_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load SimCLR Model\n",
        "simclr_model = SimCLR(linear_eval=True).to(device)  # linear_eval=True to avoid augmentation stacking\n",
        "simclr_model.load_state_dict(torch.load(simclr_path, map_location=device))\n",
        "simclr_model.eval()\n",
        "\n",
        "# Image Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def extract_image_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Pass tensor directly, not wrapped in list\n",
        "        features = simclr_model(image_tensor)\n",
        "    return features.squeeze(0)\n",
        "\n",
        "# Encode Question\n",
        "def encode_single_question(question, question_vocab, max_length=25):\n",
        "    tokens = question.lower().strip().split()\n",
        "    token_ids = [question_vocab.get(t, question_vocab.get('<UNK>', 0)) for t in tokens]\n",
        "    token_ids = token_ids[:max_length]\n",
        "    token_ids += [question_vocab['<PAD>']] * (max_length - len(token_ids))\n",
        "    return torch.tensor(token_ids).unsqueeze(0).to(device)\n",
        "\n",
        "# Prediction\n",
        "def predict_topk(image_path, question_text, k=5):\n",
        "    image_tensor = extract_image_features(image_path).unsqueeze(0)\n",
        "    question_tensor = encode_single_question(question_text, question_vocab)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor, question_tensor)\n",
        "        probs = F.softmax(output, dim=1).squeeze(0)\n",
        "\n",
        "        topk_probs, topk_indices = torch.topk(probs, k)\n",
        "        topk_answers = [idx_to_answer.get(i.item(), \"<UNK>\") for i in topk_indices]\n",
        "\n",
        "    print(f\"\\nQuestion: {question_text}\\n\")\n",
        "    print(\"Top Predictions:\")\n",
        "    for i in range(k):\n",
        "        print(f\"{i+1}. {topk_answers[i]} ({topk_probs[i].item()*100:.2f}%)\")\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Q: {question_text}\")\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "image_id = 107087\n",
        "image_filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
        "image_path = f\"/content/val2014/{image_filename}\"\n",
        "question = \"Which animal is depicted in the figure?\"\n",
        "\n",
        "predict_topk(image_path, question, k=10)"
      ],
      "metadata": {
        "id": "WZR88wWxjZy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compare Models**"
      ],
      "metadata": {
        "id": "-ZrifPGyJhTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# val_losses_lstm = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_losses_lstm.npy')\n",
        "# val_losses_lstm_att = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_losses_lstm_att.npy')\n",
        "# val_losses_cnn = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_losses_cnn.npy')\n",
        "# val_losses_rnn = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_losses_rnn.npy')\n",
        "\n",
        "# val_accs_rnn = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_accs_lstm.npy')\n",
        "# val_accs_cnn = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_accs_cnn.npy')\n",
        "# val_accs_lstm_att = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_accs_lstm_att.npy')\n",
        "# val_accs_lstm = np.load('/content/drive/MyDrive/Colab Notebooks/DL-CV-project/ResNet50-ALL/val_accs_rnn.npy')"
      ],
      "metadata": {
        "id": "Ufotohuqc9mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_losses_lstm) + 1)\n",
        "\n",
        "# Create DataFrame for Losses\n",
        "df_losses = pd.DataFrame({\n",
        "    'Epoch': epochs,\n",
        "    'LSTM': val_losses_lstm,\n",
        "    'LSTM_Attention': val_losses_lstm_att,\n",
        "    '1D_CNN': val_losses_cnn,\n",
        "    'RNN': val_losses_rnn\n",
        "})\n",
        "\n",
        "# Create DataFrame for Accuracies\n",
        "df_accs = pd.DataFrame({\n",
        "    'Epoch': epochs,\n",
        "    'LSTM': val_accs_lstm,\n",
        "    'LSTM_Attention': val_accs_lstm_att,\n",
        "    '1D_CNN': val_accs_cnn,\n",
        "    'RNN': val_accs_rnn\n",
        "})\n",
        "\n",
        "# Plot Losses\n",
        "plt.figure(figsize=(12, 5))\n",
        "for model in ['LSTM', 'LSTM_Attention', '1D_CNN', 'RNN']:\n",
        "    plt.plot(df_losses['Epoch'], df_losses[model], label=model)\n",
        "\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('val_loss_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracies\n",
        "plt.figure(figsize=(12, 5))\n",
        "for model in ['LSTM', 'LSTM_Attention', '1D_CNN', 'RNN']:\n",
        "    plt.plot(df_accs['Epoch'], df_accs[model], label=model)\n",
        "\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('val_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RraZgUPnJj0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}