{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ViT fine-tuning**"
      ],
      "metadata": {
        "id": "65u1F08vCBQ2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z7bVyDf-DFn"
      },
      "source": [
        "## **Download datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "457UClHV-CP-"
      },
      "outputs": [],
      "source": [
        "!wget -nc http://images.cocodataset.org/zips/train2014.zip\n",
        "!wget -nc http://images.cocodataset.org/zips/val2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oQgLmY2BSyL"
      },
      "outputs": [],
      "source": [
        "# Unzip all files\n",
        "!unzip -q train2014.zip\n",
        "!unzip -q val2014.zip\n",
        "\n",
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VQA v2 zip files\n",
        "!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\n",
        "!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\n",
        "!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\n",
        "!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip"
      ],
      "metadata": {
        "id": "_7u5-dZu3_is",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q v2_Questions_Train_mscoco.zip\n",
        "!unzip -q v2_Questions_Val_mscoco.zip\n",
        "!unzip -q v2_Annotations_Train_mscoco.zip\n",
        "!unzip -q v2_Annotations_Val_mscoco.zip\n",
        "\n",
        "!rm *.zip"
      ],
      "metadata": {
        "id": "ov2Z9utt4BHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import libraries**"
      ],
      "metadata": {
        "id": "uMwBnjypCWfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKTz-46uVPXc"
      },
      "outputs": [],
      "source": [
        "# standard libs\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "from glob import glob\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# TorchVision\n",
        "import torchvision\n",
        "from torchvision import transforms, utils, models\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from transformers import CLIPModel, CLIPProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-SgbDr1UwWS"
      },
      "source": [
        "## **Extract from datasets**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IsLoEbunBb7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images\n",
        "images_train_folder = '/content/train2014'\n",
        "images_val_folder = '/content/val2014'\n",
        "\n",
        "train_images = glob(os.path.join(images_train_folder, '*.jpg'))\n",
        "val_images = glob(os.path.join(images_val_folder, '*.jpg'))\n",
        "\n",
        "print(len(train_images))\n",
        "print(len(val_images))"
      ],
      "metadata": {
        "id": "y5x5GTAkUBgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce list images size\n",
        "train_images = train_images[:len(train_images)//80]\n",
        "val_images = val_images[:len(val_images)//80]\n",
        "print(len(train_images))\n",
        "print(len(val_images))"
      ],
      "metadata": {
        "id": "VFuZJXERYYcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# questions\n",
        "questions_val_json = '/content/v2_OpenEnded_mscoco_val2014_questions.json'\n",
        "questions_train_json = '/content/v2_OpenEnded_mscoco_train2014_questions.json'\n",
        "\n",
        "# annotations\n",
        "annotations_train_json = '/content/v2_mscoco_train2014_annotations.json'\n",
        "annotations_val_json = '/content/v2_mscoco_val2014_annotations.json'\n",
        "\n",
        "# images\n",
        "images_train_folder = '/content/train2014'\n",
        "images_val_folder = '/content/val2014'"
      ],
      "metadata": {
        "id": "4LK-stdF3flg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading json files as lists\n",
        "train_questions_list = json.load(open(questions_train_json))\n",
        "val_questions_list = json.load(open(questions_val_json))\n",
        "train_annotations_list = json.load(open(annotations_train_json))\n",
        "val_annotations_list = json.load(open(annotations_val_json))"
      ],
      "metadata": {
        "id": "nho3h0ME3gQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract image_id from filename\n",
        "def extract_image_id(filename):\n",
        "    base = os.path.basename(filename)  # e.g. COCO_train2014_000000123456.jpg\n",
        "    img_id_str = base.split('_')[-1].split('.')[0]  # '000000123456'\n",
        "    return int(img_id_str)"
      ],
      "metadata": {
        "id": "PUhSfwkF3tuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image IDs\n",
        "train_image_ids = set(extract_image_id(f) for f in train_images)\n",
        "val_image_ids = set(extract_image_id(f) for f in val_images)"
      ],
      "metadata": {
        "id": "Ytx29BoN31bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create lists\n",
        "train_list = train_questions_list['questions']\n",
        "val_list = val_questions_list['questions']\n",
        "\n",
        "for i in range(len(train_list)):\n",
        "    train_list[i]['multiple_choice_answer'] = train_annotations_list['annotations'][i]['multiple_choice_answer']\n",
        "    train_list[i]['answers'] = train_annotations_list['annotations'][i]['answers']\n",
        "    train_list[i]['answer_type'] = train_annotations_list['annotations'][i]['answer_type']\n",
        "\n",
        "for i in range(len(val_list)):\n",
        "    val_list[i]['multiple_choice_answer'] = val_annotations_list['annotations'][i]['multiple_choice_answer']\n",
        "    val_list[i]['answers'] = val_annotations_list['annotations'][i]['answers']\n",
        "    val_list[i]['answer_type'] = val_annotations_list['annotations'][i]['answer_type']"
      ],
      "metadata": {
        "id": "zgSnGhqp4Tn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the list to be based on the images that are /4\n",
        "filtered_train_list = [q for q in train_list if q['image_id'] in train_image_ids]\n",
        "filtered_val_list = [q for q in val_list if q['image_id'] in val_image_ids]\n",
        "\n",
        "print(\"Filtered train questions:\", len(filtered_train_list))\n",
        "print(\"Filtered val questions:\", len(filtered_val_list))"
      ],
      "metadata": {
        "id": "xQgLg0cm4d2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique image IDs\n",
        "filtered_train_image_ids = set(q['image_id'] for q in filtered_train_list)\n",
        "\n",
        "# check\n",
        "print(\"Train image IDs from images:\", len(train_image_ids))\n",
        "print(\"Train image IDs from filtered questions:\", len(filtered_train_image_ids))\n",
        "print(\"Are train image ID sets equal?\", train_image_ids == filtered_train_image_ids)"
      ],
      "metadata": {
        "id": "l6Ttdkm358tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create folder\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "new_folder = 'ViT_Embeddings'\n",
        "full_path = os.path.join(drive_path, new_folder)\n",
        "\n",
        "if not os.path.exists(full_path):\n",
        "    os.makedirs(full_path)\n",
        "    print(f\"Folder '{new_folder}' created at {full_path}\")\n"
      ],
      "metadata": {
        "id": "zJ567arRgcjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ViT implementation**"
      ],
      "metadata": {
        "id": "-k0QWQCk9__a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create dataset**"
      ],
      "metadata": {
        "id": "_Gz7QMMNCtbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, list_images, transform=None):\n",
        "        self.list_images = list_images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.list_images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image"
      ],
      "metadata": {
        "id": "arTD1709cP5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define transformations for images**"
      ],
      "metadata": {
        "id": "Ihrgb3IuC3bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://medium.com/@prabowoyogawicaksana/self-supervised-pre-training-with-simclr-79830997be34\n",
        "\n",
        "# If using .jpg or .png files for training:\n",
        "train_image_paths = train_images\n",
        "val_image_paths = val_images\n",
        "\n",
        "# ------------ TRANSFORMS ------------\n",
        "def get_complete_transform(output_shape, s=1.0):\n",
        "    rnd_crop = transforms.RandomResizedCrop(output_shape, scale=(0.3, 1.0))\n",
        "    rnd_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
        "\n",
        "    color_jitter = transforms.ColorJitter(0.4 * s, 0.4 * s, 0.4 * s, 0.1 * s)\n",
        "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "\n",
        "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "    gaussian_blur = transforms.GaussianBlur(kernel_size=23)\n",
        "    rnd_gaussian_blur = transforms.RandomApply([gaussian_blur], p=0.5)\n",
        "\n",
        "    clip_normalize = transforms.Normalize(\n",
        "        mean=[0.4815, 0.4578, 0.4082],\n",
        "        std=[0.2686, 0.2613, 0.2758]\n",
        "    )\n",
        "\n",
        "    image_transform = transforms.Compose([\n",
        "        rnd_crop,\n",
        "        rnd_flip,\n",
        "        rnd_color_jitter,\n",
        "        rnd_gray,\n",
        "        rnd_gaussian_blur,\n",
        "        transforms.ToTensor(),\n",
        "        clip_normalize\n",
        "    ])\n",
        "    return image_transform\n",
        "\n",
        "class ContrastiveLearningViewGenerator:\n",
        "    \"\"\"\n",
        "    Generates multiple augmented views of the same image.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_transform, n_views=2):\n",
        "        self.base_transform = base_transform\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __call__(self, x):\n",
        "        views = [self.base_transform(x) for _ in range(self.n_views)]\n",
        "        return views\n"
      ],
      "metadata": {
        "id": "IW6ZfDvvcMuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss function**"
      ],
      "metadata": {
        "id": "wx3ejnH-DDaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOSS FUNCTION\n",
        "\n",
        "def ntxent_loss(features, temp=0.5):\n",
        "    batch_size = features.shape[0]\n",
        "    assert batch_size % 2 == 0, \"Batch size should be even for SimCLR.\"\n",
        "\n",
        "    # Create labels dynamically for the current batch size\n",
        "    labels = torch.cat([torch.arange(batch_size // 2) for _ in range(2)], dim=0)\n",
        "    labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float().to(DEVICE) # one-hot similarity mask\n",
        "\n",
        "    similarity_matrix = torch.matmul(features, features.T)\n",
        "\n",
        "    mask = torch.eye(batch_size, dtype=torch.bool).to(DEVICE)\n",
        "\n",
        "    labels = labels[~mask].view(batch_size, -1)\n",
        "    similarity_matrix = similarity_matrix[~mask].view(batch_size, -1)\n",
        "\n",
        "    positives = similarity_matrix[labels.bool()].view(batch_size, -1)\n",
        "    negatives = similarity_matrix[~labels.bool()].view(batch_size, -1)\n",
        "\n",
        "    logits = torch.cat([positives, negatives], dim=1)\n",
        "    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    logits = logits / temp\n",
        "    return logits, labels\n"
      ],
      "metadata": {
        "id": "tj-GWWfScV8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ViT model**"
      ],
      "metadata": {
        "id": "6d0TxBhPCv73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def _init_(self):\n",
        "        super(Identity, self)._init_()\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, linear_eval=False, unfreeze_vit=True):\n",
        "        \"\"\"\n",
        "        CLIP ViT-L/14 feature extractor with projection head.\n",
        "        Args:\n",
        "            linear_eval (bool): If True, disables contrastive view stacking.\n",
        "            unfreeze_vit (bool): If True, unfreezes some transformer layers.\n",
        "        \"\"\"\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.linear_eval = linear_eval\n",
        "\n",
        "        # Load Hugging Face CLIP ViT-L/14\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "        self.clip_model.eval()\n",
        "\n",
        "        # Freeze all by default\n",
        "        for param in self.clip_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze last few layers\n",
        "        if unfreeze_vit:\n",
        "            for name, param in self.clip_model.vision_model.named_parameters():\n",
        "                if \"encoder.layers.21\" in name or \\\n",
        "                   \"encoder.layers.22\" in name or \\\n",
        "                   \"post_layernorm\" in name:\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        # Extract the vision encoder\n",
        "        self.encoder = self.clip_model.vision_model\n",
        "\n",
        "        # Define projection head\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: list of views [view1_tensor, view2_tensor] if not linear_eval\n",
        "               or single tensor if linear_eval\n",
        "        Returns:\n",
        "            Normalized projection vector (N, 128)\n",
        "        \"\"\"\n",
        "        if not self.linear_eval:\n",
        "            x = torch.cat(x, dim=0)\n",
        "\n",
        "        # Forward through vision transformer\n",
        "        outputs = self.encoder(pixel_values=x)\n",
        "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        projection = self.projection(cls_token)\n",
        "        return F.normalize(projection, dim=1)\n",
        "\n",
        "    def get_preprocess(self):\n",
        "        return self.preprocess"
      ],
      "metadata": {
        "id": "qPGUK4oObt-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ViT training**"
      ],
      "metadata": {
        "id": "J0PY9ElKDQQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG\n",
        "\n",
        "BATCH_SZ = 64\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "out_shape = 224\n",
        "num_workers = 2\n",
        "EPOCHS = 10\n",
        "model_filename = 'ViT_def.pth'\n",
        "best_val_loss = float('inf')  # initialize best loss\n",
        "\n",
        "# Transforms and Dataset\n",
        "base_transforms = get_complete_transform(output_shape=out_shape)\n",
        "custom_transform = ContrastiveLearningViewGenerator(base_transform=base_transforms)\n",
        "\n",
        "ds_train = CustomDataset(list_images=train_image_paths, transform=custom_transform)\n",
        "ds_val = CustomDataset(list_images=val_image_paths, transform=custom_transform)\n",
        "\n",
        "train_dl = DataLoader(ds_train, batch_size=BATCH_SZ, num_workers=num_workers, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_dl = DataLoader(ds_val, batch_size=BATCH_SZ, num_workers=num_workers, shuffle=False, drop_last=False, pin_memory=True)\n",
        "\n",
        "# Model & Optimizer\n",
        "model = SimCLR().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# TRAINING LOOP\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    # TRAIN\n",
        "    model.train()\n",
        "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "    for batch_idx, views in enumerate(train_dl):\n",
        "        projections = model([view.to(DEVICE) for view in views])\n",
        "        logits, labels = ntxent_loss(projections)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        print(f\"Train -> Batch {batch_idx+1}/{len(train_dl)} - Loss: {loss.item():.4f} - Acc: {(preds == labels).float().mean().item():.4f}\")\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dl)\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # VAL\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, views in enumerate(val_dl):\n",
        "            projections = model([view.to(DEVICE) for view in views])\n",
        "            logits, labels = ntxent_loss(projections)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            print(f\"Val -> Batch {batch_idx+1}/{len(val_dl)} - Loss: {loss.item():.4f} - Acc: {(preds == labels).float().mean().item():.4f}\")\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dl)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Save Model with the lowest val loss\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(full_path, model_filename))\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\nEpoch {epoch+1}:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {avg_val_loss:.4f} | Val   Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "-ith1f79MjCa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot results**"
      ],
      "metadata": {
        "id": "gys5eulTDXmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_plot.png')  # Save plot to file\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_accuracies, label='Train Accuracy', marker='o')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('accuracy_plot.png')  # Save plot to file\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hm3dPh081_uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cosine_similarity(model, dataloader, device, save_path=\"cosine_similarity.png\"):\n",
        "    model.eval()\n",
        "    cosine_sims = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for views in dataloader:\n",
        "            view1, view2 = views[0].to(device), views[1].to(device)\n",
        "            if view1.dim() == 3:\n",
        "                view1 = view1.unsqueeze(0)\n",
        "            if view2.dim() == 3:\n",
        "                view2 = view2.unsqueeze(0)\n",
        "\n",
        "            z1 = model([view1])\n",
        "            z2 = model([view2])\n",
        "            sim = torch.nn.functional.cosine_similarity(z1, z2)\n",
        "            cosine_sims.extend(sim.cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.histplot(cosine_sims, bins=50, kde=True, color=\"purple\")\n",
        "    plt.title(\"Cosine Similarity of Positive Pairs\")\n",
        "    plt.xlabel(\"Cosine Similarity\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)  # Save plot to file\n",
        "    plt.show()\n",
        "\n",
        "# plot\n",
        "plot_cosine_similarity(model, val_dl, DEVICE)"
      ],
      "metadata": {
        "id": "cgIk15T2d3Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Save all images**"
      ],
      "metadata": {
        "id": "Tddux9wvW4Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# images\n",
        "train_images = glob(os.path.join(images_train_folder, '*.jpg'))\n",
        "val_images = glob(os.path.join(images_val_folder, '*.jpg'))\n",
        "\n",
        "print(len(train_images))\n",
        "print(len(val_images))"
      ],
      "metadata": {
        "id": "6NLb8CTyW90d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image IDs\n",
        "train_image_ids = set(extract_image_id(f) for f in train_images)\n",
        "val_image_ids = set(extract_image_id(f) for f in val_images)"
      ],
      "metadata": {
        "id": "poLMcPvceF08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create lists\n",
        "train_list = train_questions_list['questions']\n",
        "val_list = val_questions_list['questions']\n",
        "\n",
        "for i in range(len(train_list)):\n",
        "    train_list[i]['multiple_choice_answer'] = train_annotations_list['annotations'][i]['multiple_choice_answer']\n",
        "    train_list[i]['answers'] = train_annotations_list['annotations'][i]['answers']\n",
        "    train_list[i]['answer_type'] = train_annotations_list['annotations'][i]['answer_type']\n",
        "\n",
        "for i in range(len(val_list)):\n",
        "    val_list[i]['multiple_choice_answer'] = val_annotations_list['annotations'][i]['multiple_choice_answer']\n",
        "    val_list[i]['answers'] = val_annotations_list['annotations'][i]['answers']\n",
        "    val_list[i]['answer_type'] = val_annotations_list['annotations'][i]['answer_type']"
      ],
      "metadata": {
        "id": "G7Kj5XdmeIsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_train_list = [q for q in train_list if q['image_id'] in train_image_ids]\n",
        "filtered_val_list = [q for q in val_list if q['image_id'] in val_image_ids]\n",
        "\n",
        "print(\"Filtered train questions:\", len(filtered_train_list))\n",
        "print(\"Filtered val questions:\", len(filtered_val_list))"
      ],
      "metadata": {
        "id": "3L-Jb8ISeIzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique image IDs\n",
        "filtered_train_image_ids = set(q['image_id'] for q in filtered_train_list)\n",
        "\n",
        "# check\n",
        "print(\"Train image IDs from images:\", len(train_image_ids))\n",
        "print(\"Train image IDs from filtered questions:\", len(filtered_train_image_ids))\n",
        "print(\"Are train image ID sets equal?\", train_image_ids == filtered_train_image_ids)"
      ],
      "metadata": {
        "id": "OsrHRKKieI4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save train_list\n",
        "train_list_path = os.path.join(full_path, 'train_list.pkl')\n",
        "with open(train_list_path, 'wb') as f:\n",
        "    pickle.dump(filtered_train_list, f)\n",
        "print(f\"Saved train_list to {train_list_path}\")\n",
        "\n",
        "# save val_list\n",
        "val_list_path = os.path.join(full_path, 'val_list.pkl')\n",
        "with open(val_list_path, 'wb') as f:\n",
        "    pickle.dump(filtered_val_list, f)\n",
        "print(f\"Saved val_list to {val_list_path}\")"
      ],
      "metadata": {
        "id": "Hs3A-6lQW8az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extract features**"
      ],
      "metadata": {
        "id": "M7gf6RmoS6uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load your model\n",
        "model = SimCLR(unfreeze_vit=False, linear_eval=True) # model.linear_eval = True   to avoid Concatenation\n",
        "\n",
        "state_dict = torch.load('/content/drive/MyDrive/ViT_Embeddings/ViT_def.pth', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "id": "XPDm69SOfORp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset for JPG images\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, json_list, image_folder, split=\"train2014\"):\n",
        "        self.imgids = list(set([i['image_id'] for i in json_list]))\n",
        "        self.image_folder = image_folder  # e.g., '/content/train2014'\n",
        "        self.split = split                # 'train2014' or 'val2014'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.imgids[idx]\n",
        "        filename = f\"COCO_{self.split}_{str(img_id).rjust(12, '0')}.jpg\"\n",
        "        file_path = os.path.join(self.image_folder, filename)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"Image not found: {file_path}\")\n",
        "\n",
        "        image = Image.open(file_path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        return {\"image\": image, \"id\": img_id}"
      ],
      "metadata": {
        "id": "wXaBaPM3fS9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction function\n",
        "def extract_and_save_features(json_list, image_folder, split_name, batch_size=256, num_workers=2):\n",
        "    dataset = ImageDataset(json_list, image_folder, split=split_name)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True)\n",
        "\n",
        "    model.eval()\n",
        "    model.linear_eval = True\n",
        "    model.to('cuda')\n",
        "\n",
        "    all_features = []\n",
        "    all_img_ids = []\n",
        "\n",
        "    for batch in tqdm(loader, desc=f\"Extracting features for {split_name}\"):\n",
        "        images = batch['image'].to('cuda')\n",
        "        img_ids = batch['id']\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model(images)\n",
        "\n",
        "        features_np = features.cpu().numpy().astype('float16')\n",
        "        all_features.append(features_np)\n",
        "        all_img_ids.extend(img_ids)\n",
        "\n",
        "    all_features = np.vstack(all_features)\n",
        "    all_img_ids = np.array(all_img_ids, dtype=np.int32)\n",
        "\n",
        "    # save\n",
        "    save_path = os.path.join(full_path, f\"{split_name}_image_features.pkl\")\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump({\"img_ids\": all_img_ids, \"features\": all_features}, f)\n",
        "\n",
        "    print(f\"Saved {len(all_img_ids)} features to {save_path}\")"
      ],
      "metadata": {
        "id": "v2SP5artfaFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgids = list(set([i['image_id'] for i in filtered_train_list]))\n",
        "print(len(imgids))"
      ],
      "metadata": {
        "id": "_L2GUL4UpIxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# extract\n",
        "extract_and_save_features(filtered_train_list, \"/content/train2014\", \"train2014\")\n",
        "extract_and_save_features(filtered_val_list, \"/content/val2014\", \"val2014\")"
      ],
      "metadata": {
        "id": "3P3XFN7-bJ6x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}